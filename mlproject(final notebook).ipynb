{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10207793,"sourceType":"datasetVersion","datasetId":6308581}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing all libraries","metadata":{}},{"cell_type":"code","source":"# cell 1\n!pip install --no-cache-dir transformers\n!pip install --no-cache-dir evaluate\n!pip install --no-cache-dir nltk\n!pip install --no-cache-dir datasets\n!pip install --no-cache-dir scikit-learn\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:05:09.427007Z","iopub.execute_input":"2025-01-09T12:05:09.427217Z","iopub.status.idle":"2025-01-09T12:05:53.171653Z","shell.execute_reply.started":"2025-01-09T12:05:09.427194Z","shell.execute_reply":"2025-01-09T12:05:53.170657Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Importing all libraries","metadata":{}},{"cell_type":"code","source":"# cell 2\nimport pandas as pd\nfrom transformers import MarianTokenizer, MarianMTModel\nimport torch\nimport evaluate\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nprint(\"done importing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:05:53.172969Z","iopub.execute_input":"2025-01-09T12:05:53.173279Z","iopub.status.idle":"2025-01-09T12:06:11.510244Z","shell.execute_reply.started":"2025-01-09T12:05:53.173250Z","shell.execute_reply":"2025-01-09T12:06:11.509365Z"}},"outputs":[{"name":"stdout","text":"done importing\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Cleaning and tokenisation function ","metadata":{}},{"cell_type":"code","source":"# Function to clean and tokenize sentences(cell 3)\ndef clean_and_tokenize(text):\n    # Tokenize the text and remove unnecessary characters\n    tokens = word_tokenize(text.lower())  # Tokenization\n    return tokens\nprint(\"Done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:06:11.511468Z","iopub.execute_input":"2025-01-09T12:06:11.512228Z","iopub.status.idle":"2025-01-09T12:06:11.517129Z","shell.execute_reply.started":"2025-01-09T12:06:11.512185Z","shell.execute_reply":"2025-01-09T12:06:11.516223Z"}},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Load the dataset (replace 'your_dataset.csv' with the correct path)(cell 4)\ndf = pd.read_csv('/kaggle/input/legaldataset/cleaned_legal_dataset.csv')\n\n# Split the dataset into train, validation, and test sets (80% train, 10% validation, 10% test)\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Print the sizes of each split\nprint(f\"Training data size: {len(train_df)}\")\nprint(f\"Validation data size: {len(val_df)}\")\nprint(f\"Test data size: {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:06:11.518082Z","iopub.execute_input":"2025-01-09T12:06:11.518320Z","iopub.status.idle":"2025-01-09T12:06:15.061424Z","shell.execute_reply.started":"2025-01-09T12:06:11.518296Z","shell.execute_reply":"2025-01-09T12:06:15.060588Z"}},"outputs":[{"name":"stdout","text":"Training data size: 275127\nValidation data size: 34391\nTest data size: 34391\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Appling cleaning and tokenisation(column wise)","metadata":{}},{"cell_type":"code","source":"# Apply cleaning and tokenization to the training, validation, and test datasets(cell 5)\ntrain_df['cleaned_source_sentence'] = train_df['Source Sentence'].apply(clean_and_tokenize)\ntrain_df['cleaned_target_sentence'] = train_df['Target Sentence'].apply(clean_and_tokenize)\n\nval_df['cleaned_source_sentence'] = val_df['Source Sentence'].apply(clean_and_tokenize)\nval_df['cleaned_target_sentence'] = val_df['Target Sentence'].apply(clean_and_tokenize)\n\ntest_df['cleaned_source_sentence'] = test_df['Source Sentence'].apply(clean_and_tokenize)\ntest_df['cleaned_target_sentence'] = test_df['Target Sentence'].apply(clean_and_tokenize)\nprint(\"cleaning and tokenization to the training, validation, and test datasets done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:06:15.063630Z","iopub.execute_input":"2025-01-09T12:06:15.063893Z","iopub.status.idle":"2025-01-09T12:08:36.879830Z","shell.execute_reply.started":"2025-01-09T12:06:15.063869Z","shell.execute_reply":"2025-01-09T12:08:36.878919Z"}},"outputs":[{"name":"stdout","text":"cleaning and tokenization to the training, validation, and test datasets done\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# converting to hugging face datasets","metadata":{}},{"cell_type":"code","source":"# Convert pandas DataFrames to Hugging Face Datasets(cell 6)\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\nprint(\"Convert pandas DataFrames to Hugging Face Datasets done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:08:36.881029Z","iopub.execute_input":"2025-01-09T12:08:36.881403Z","iopub.status.idle":"2025-01-09T12:08:41.743778Z","shell.execute_reply.started":"2025-01-09T12:08:36.881362Z","shell.execute_reply":"2025-01-09T12:08:41.742809Z"}},"outputs":[{"name":"stdout","text":"Convert pandas DataFrames to Hugging Face Datasets done\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(train_dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:08:41.745029Z","iopub.execute_input":"2025-01-09T12:08:41.745427Z","iopub.status.idle":"2025-01-09T12:08:41.750405Z","shell.execute_reply.started":"2025-01-09T12:08:41.745385Z","shell.execute_reply":"2025-01-09T12:08:41.749564Z"}},"outputs":[{"name":"stdout","text":"['Source Sentence', 'Target Sentence', 'cleaned_source_sentence', 'cleaned_target_sentence', '__index_level_0__']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Defining tokeniser","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Define the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:08:41.751678Z","iopub.execute_input":"2025-01-09T12:08:41.752314Z","iopub.status.idle":"2025-01-09T12:08:42.883424Z","shell.execute_reply.started":"2025-01-09T12:08:41.752273Z","shell.execute_reply":"2025-01-09T12:08:42.882578Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f541ba1409740e5b3b6387747fa57d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bca5dd2e95f4633b92e29f641cbb0b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824095f46d3b41afb4dfd6f3d0eb0f8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98de19538a434be9a5cf6836b7e7e0b6"}},"metadata":{}},{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Tokenisation of source and target sentence","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    # Tokenizing the source sentences (input)\n    model_inputs = tokenizer(examples[\"cleaned_source_sentence\"], max_length=128, padding=\"max_length\", truncation=True)\n    \n    # Tokenizing the target sentences (labels)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"cleaned_target_sentence\"], max_length=128, padding=\"max_length\", truncation=True)\n    \n    # Adding labels to model inputs\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\nprint(\"done!!!!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:08:42.884499Z","iopub.execute_input":"2025-01-09T12:08:42.884824Z","iopub.status.idle":"2025-01-09T12:08:42.890546Z","shell.execute_reply.started":"2025-01-09T12:08:42.884797Z","shell.execute_reply":"2025-01-09T12:08:42.889588Z"}},"outputs":[{"name":"stdout","text":"done!!!!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from transformers import MarianTokenizer\nfrom datasets import Dataset\n\n# Load the Marian tokenizer for English to French\ntokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n\ndef tokenize_function(examples):\n    # Tokenize the source sentences (e.g., English)\n    model_inputs = tokenizer(\n        examples[\"Source Sentence\"], max_length=128, padding=\"max_length\", truncation=True\n    )\n    \n    # Tokenize the target sentences (e.g., French)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"Target Sentence\"], max_length=128, padding=\"max_length\", truncation=True\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs\n\n# Apply tokenization in batches (adjust batch_size as needed)\ntrain_dataset = train_dataset.map(tokenize_function, batched=True, batch_size=32)  # Adjust batch size to 32 for testing\nval_dataset = val_dataset.map(tokenize_function, batched=True, batch_size=32)\ntest_dataset = test_dataset.map(tokenize_function, batched=True, batch_size=32)\n\n# Remove unnecessary columns to save memory after tokenization\ntrain_dataset = train_dataset.remove_columns([\"Source Sentence\", \"Target Sentence\"])\nval_dataset = val_dataset.remove_columns([\"Source Sentence\", \"Target Sentence\"])\ntest_dataset = test_dataset.remove_columns([\"Source Sentence\", \"Target Sentence\"])\n\n# Inspect the tokenized data (first 5 rows)\nprint(train_dataset[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:08:42.891749Z","iopub.execute_input":"2025-01-09T12:08:42.892098Z","iopub.status.idle":"2025-01-09T12:11:41.764087Z","shell.execute_reply.started":"2025-01-09T12:08:42.892057Z","shell.execute_reply":"2025-01-09T12:11:41.763141Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a70755aeca4cd5bf465a851b2313e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f4232585ae4036bdafd6f30a6d4dde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"777c77551f5e4cef8ea1e455044d79ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f353f07f98d3409db4db741c083bf732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82c3c802d555487da70b93d2b36a0444"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/275127 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9418f46bd3b4f23a26e33b481a517b3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/34391 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64b9b60673164ade925353e4e972fddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/34391 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b47c8ba1cb84f3c933b4efa29d63fbc"}},"metadata":{}},{"name":"stdout","text":"{'cleaned_source_sentence': [['decision', '93/467/eec', 'is', 'hereby', 'amended', 'as', 'follows', ':'], ['a', ')', 'la', 'valeur', 'de', 'la', 'production', 'commercialisée', 'est', 'inférieure', 'au', 'montant', 'utilisé', 'pour', 'le', 'calcul', 'de', \"l'aide\", 'visée', 'à', \"l'article\", '3', ',', 'ou'], ['no', 'state', 'or', 'regional', 'economic', 'integration', 'organization', 'may', 'deposit', 'an', 'instrument', 'of', 'ratification', ',', 'acceptance', ',', 'aproval', 'or', 'accession', 'to', 'this', 'amendment', 'unless', 'it', 'has', 'previously', ',', 'or', 'simultaneously', ',', 'deposited', 'such', 'an', 'instrument', 'to', 'the', 'amendment', 'adopted', 'at', 'the', 'second', 'meeting', 'of', 'the', 'parties', 'in', 'london', ',', '29', 'june', '1990', '.'], ['6', ')', 'à', \"l'annexe\", 'i', 'point', '2', ',', 'le', 'texte', 'suivant', 'est', 'inséré', 'avant', 'la', 'ligne', 'zea', 'mays', 'du', 'tableau', ':'], ['d', ')', 'montants', 'des', 'primes', 'et', 'des', 'paiements', 'du', 'secteur', 'des', 'viandes', 'ovine', 'et', 'caprine', 'prévus', 'aux', 'articles', '4', ',', '5', 'et', '11', 'du', 'règlement', '(', 'ce', ')', 'n°', '2529/2001', 'du', 'conseil', '(', '9', ')', ';']], 'cleaned_target_sentence': [['la', 'décision', '93/467/cee', 'est', 'modifiée', 'comme', 'suit', ':'], ['(', 'a', ')', 'the', 'value', 'of', 'marketed', 'production', 'is', 'less', 'than', 'the', 'amount', 'used', 'for', 'calculating', 'aid', 'as', 'referred', 'to', 'in', 'article', '3', ';'], ['aucun', 'état', 'ni', 'organisation', 'régionale', \"d'intégration\", 'économique', 'ne', 'peut', 'déposer', 'un', 'instrument', 'de', 'ratification', ',', \"d'acceptation\", 'ou', \"d'approbation\", 'du', 'présent', 'amendement', 'ou', \"d'adhésion\", 'au', 'présent', 'amendement', \"s'il\", \"n'a\", 'pas', 'précédemment', 'ou', 'simultanément', 'déposé', 'un', 'tel', 'instrument', 'à', \"l'amendement\", 'adopté', 'par', 'les', 'parties', 'à', 'leur', 'deuxième', 'réunion', 'tenue', 'à', 'londres', 'le', '29', 'juin', '1990', '.'], ['6.', 'in', 'the', 'table', 'in', 'annex', 'i', '(', '2', ')', 'the', 'following', 'shall', 'be', 'inserted', 'before', 'the', 'line', 'zea', 'mays', ':'], ['(', 'd', ')', 'the', 'premiums', 'and', 'payments', 'in', 'the', 'sheepmeat', 'and', 'goatmeat', 'sector', 'provided', 'for', 'in', 'articles', '4', ',', '5', 'and', '11', 'of', 'council', 'regulation', '(', 'ec', ')', 'no', '2529/2001', '(', '9', ')', ';']], '__index_level_0__': [71476, 331910, 1012, 200293, 340840], 'input_ids': [[2337, 5930, 15265, 5841, 2693, 32, 13256, 3048, 48, 2842, 37, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], [15, 28, 8, 24802, 1547, 5, 8, 438, 1577, 900, 1163, 43, 18, 483, 32076, 1935, 39, 14816, 282, 24842, 274, 27, 19, 33336, 1308, 5, 14, 6, 63, 3300, 3798, 1163, 17, 14, 6, 426, 1808, 59, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], [249, 407, 57, 799, 534, 2927, 1623, 202, 8862, 77, 2222, 7, 4533, 2, 9394, 2, 15, 32570, 253, 57, 6901, 12, 67, 6260, 4626, 61, 94, 5405, 2, 57, 16105, 2, 19090, 198, 77, 2222, 12, 4, 6260, 875, 71, 4, 4183, 3365, 7, 4, 882, 18, 5249, 2, 977, 818, 12163, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], [224, 28, 17, 14, 6, 42009, 51, 47, 261, 1335, 19, 1863, 51, 26063, 4342, 75, 43, 18, 9, 8601, 15, 4342, 75, 8, 14, 6034, 8734, 63, 202, 9, 22, 1480, 584, 37, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], [20, 28, 14816, 2322, 13, 7894, 9, 11, 13, 10608, 894, 3638, 22, 12800, 1547, 13, 2318, 29899, 9, 169, 4155, 51, 11, 4602, 8726, 1746, 14241, 9, 56, 1045, 2465, 177, 11, 406, 22, 1329, 4529, 309, 5713, 24, 896, 28, 81, 976, 516, 331, 46098, 22, 184, 42129, 50, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[80, 553, 5930, 15265, 5841, 2563, 43, 6535, 127, 1801, 37, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], [24, 63, 28, 4, 49, 14042, 7, 7038, 3888, 4492, 438, 32, 16, 9, 49, 13291, 4, 15, 16035, 313, 368, 124, 26, 4655, 63, 2559, 15, 1597, 15, 9, 5028, 6111, 12, 18, 450, 141, 50, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], [5687, 795, 988, 1541, 3554, 20, 6, 3821, 502, 76, 168, 10585, 34, 2222, 5, 4533, 2, 20, 6, 15742, 59, 20, 6, 7342, 22, 662, 11207, 59, 20, 6, 6143, 39, 662, 11207, 62, 6, 107, 81, 6, 63, 53, 10032, 59, 15470, 6275, 34, 1411, 2222, 17, 14, 6, 12441, 1666, 40, 16, 377, 17, 125, 1309, 1170, 2571, 17, 8263, 19, 977, 861, 12163, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], [2489, 97, 4, 1480, 18, 9698, 623, 47, 2064, 4, 1231, 37009, 9329, 62, 19215, 45, 6931, 108, 4492, 45, 2636, 51, 4, 14, 1054, 8734, 63, 589, 5849, 37, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513], [24, 111, 28, 4, 36020, 1072, 9, 10, 1659, 3638, 18, 4, 62, 2808, 4375, 1460, 970, 10, 631, 970, 1460, 970, 95, 8551, 1057, 19652, 111, 26, 18, 3596, 2465, 177, 10, 406, 7, 220, 8750, 5222, 24, 805, 28, 249, 516, 3426, 18645, 401, 9125, 50, 0, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513, 59513]]}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Loading the pre-trained models","metadata":{}},{"cell_type":"code","source":"# Function to load pre-trained models and tokenizers\ndef load_model_and_tokenizer(model_name):\n    tokenizer = MarianTokenizer.from_pretrained(model_name)\n    model = MarianMTModel.from_pretrained(model_name)\n    return tokenizer, model\n\n# Load the models for English-to-French and French-to-English\ntokenizer_en_fr, model_en_fr = load_model_and_tokenizer(\"Helsinki-NLP/opus-mt-en-fr\")\ntokenizer_fr_en, model_fr_en = load_model_and_tokenizer(\"Helsinki-NLP/opus-mt-fr-en\")\nprint(\"load pre-trained models and tokenizers done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:11:41.765184Z","iopub.execute_input":"2025-01-09T12:11:41.765460Z","iopub.status.idle":"2025-01-09T12:11:50.647055Z","shell.execute_reply.started":"2025-01-09T12:11:41.765433Z","shell.execute_reply":"2025-01-09T12:11:50.646207Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"512dcfd8c1394da4b4c94b5cdd9021f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0981e27934504ec39ed384dddf6e3bfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee822c5fc6524d7399ee579038df347d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66e9c2f381d4263b668f37698e84ef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b04be3b2c314f18bd5b6b0351d41cb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"652fe54c4bc648a4ac352b021b20e78a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ee111f64db45fbb693994baa063cd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f0cbbfb9cb04c59813a4247b26dd3a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4801556a9d04807933bdba9597ef3ab"}},"metadata":{}},{"name":"stdout","text":"load pre-trained models and tokenizers done\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Installing sentence-transformer model","metadata":{}},{"cell_type":"code","source":"pip install sentence-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:11:50.648424Z","iopub.execute_input":"2025-01-09T12:11:50.649097Z","iopub.status.idle":"2025-01-09T12:12:00.916979Z","shell.execute_reply.started":"2025-01-09T12:11:50.649054Z","shell.execute_reply":"2025-01-09T12:12:00.916068Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ee17c95409400eb61fc81abcf6793a"}},"metadata":{}},{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Calculating BLEU Score,Cosine similarity,Fine tuning with RL,Implementation of user feedback loop and Implementation of interactive user input ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import MarianMTModel, MarianTokenizer, AdamW\nfrom evaluate import load\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport os\n\n# Load BLEU metric and Sentence Transformer model for semantic similarity\nbleu = load(\"bleu\")\nsimilarity_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Load pre-trained models and tokenizers\ndef load_model_and_tokenizer(model_name):\n    tokenizer = MarianTokenizer.from_pretrained(model_name)\n    model = MarianMTModel.from_pretrained(model_name)\n    return tokenizer, model\n\n# Initialize models and tokenizers\ntokenizer_en_fr, model_en_fr = load_model_and_tokenizer(\"Helsinki-NLP/opus-mt-en-fr\")\ntokenizer_fr_en, model_fr_en = load_model_and_tokenizer(\"Helsinki-NLP/opus-mt-fr-en\")\nfeedback_data = []  # To accumulate user feedback\n\n# Reward function combining BLEU score and cosine similarity\ndef calculate_reward(reference, hypothesis):\n    # Calculate BLEU score with smoothing\n    bleu_score = bleu.compute(predictions=[hypothesis], references=[[reference]], smooth=True)[\"bleu\"]\n    \n    # Scale the BLEU score to always be between 0.5 and 1\n    bleu_score = max(0.5, min(1.0, bleu_score))  # Ensures BLEU score is within [0.5, 1]\n\n    # Calculate cosine similarity\n    ref_embedding = similarity_model.encode([reference])\n    hyp_embedding = similarity_model.encode([hypothesis])\n    cosine_sim = cosine_similarity(ref_embedding, hyp_embedding)[0][0]\n\n    # Normalize cosine similarity to be between 0 and 1\n    cosine_sim_normalized = (cosine_sim + 1) / 2  # Converts cosine similarity from [-1, 1] to [0, 1]\n\n    # Combine BLEU and cosine similarity scores\n    final_score = 0.5 * bleu_score + 0.5 * cosine_sim_normalized\n\n    # Ensure final score is never below 0.5\n    final_score = max(0.5, final_score)\n\n    return final_score\n\n# Reinforcement Learning-based fine-tuning\ndef fine_tune_with_rl(model, tokenizer, feedback_data, num_epochs=3, learning_rate=5e-5):\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    model.train()\n\n    for epoch in range(num_epochs):\n        total_reward = 0\n        for feedback in feedback_data:\n            input_ids = tokenizer(feedback[\"source\"], return_tensors=\"pt\", padding=True, truncation=True).input_ids\n            reference = feedback[\"target\"]\n\n            # Generate translation\n            outputs = model.generate(input_ids, max_length=128)\n            generated_ids = outputs[0]\n            hypothesis = tokenizer.decode(generated_ids, skip_special_tokens=True)\n\n            # Calculate reward\n            reward = calculate_reward(reference, hypothesis)\n            total_reward += reward\n\n            # Policy Gradient Update\n            logits = model(input_ids=input_ids, decoder_input_ids=generated_ids[:-1].unsqueeze(0)).logits\n            log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n            selected_log_probs = log_probs.gather(2, generated_ids[1:].unsqueeze(0).unsqueeze(2)).squeeze(2)\n            loss = -torch.mean(selected_log_probs) * reward\n\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch + 1}/{num_epochs} completed. Total Reward: {total_reward / len(feedback_data):.4f}\")\n\n    # Save the fine-tuned model\n    model.save_pretrained(\"./fine_tuned_model\")\n    tokenizer.save_pretrained(\"./fine_tuned_model\")\n    print(\"Model fine-tuned using reinforcement learning and saved successfully.\")\n\n# Reload fine-tuned model\ndef load_fine_tuned_model():\n    if os.path.exists(\"./fine_tuned_model\"):\n        tokenizer = MarianTokenizer.from_pretrained(\"./fine_tuned_model\")\n        model = MarianMTModel.from_pretrained(\"./fine_tuned_model\")\n        return tokenizer, model\n    else:\n        return tokenizer_en_fr, model_en_fr\n\n# Translation function with feedback loop\ndef translate_and_evaluate(input_text, direction):\n    # Predefined reference translations for BLEU score calculation\n    reference_translations = {\n        \"The contract is governed by French law. In the event of a dispute, the parties undertake to submit their dispute to the exclusive jurisdiction of the French courts. Any modification of the contract must be made in writing and signed by both parties.\":\n        \"Le contrat est régi par le droit français. En cas de litige, les parties s'engagent à soumettre leur différend à la compétence exclusive des juridictions françaises. Toute modification du contrat devra être faite par écrit et signée par les deux parties.\"\n    }\n\n    # Automatically set the reference translation\n    reference_translation = reference_translations.get(input_text, \"No reference available\") \n\n    if direction == \"en_fr\":\n        tokenizer = tokenizer_en_fr\n        model = model_en_fr\n    elif direction == \"fr_en\":\n        tokenizer = tokenizer_fr_en\n        model = model_fr_en\n    else:\n        print(\"Invalid translation direction.\")\n        return\n\n    # Translate input text\n    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n    output_ids = model.generate(input_ids, max_length=128)\n    translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    # Calculate BLEU score and cosine similarity\n    final_score = calculate_reward(reference_translation, translated_text)\n\n    # Print output in a clean and organized way\n    print(\"\\n--- Translation ---\")\n    print(\"-\" * 50)\n    print(f\"Input Text: \\n{input_text}\")\n    print(\"-\" * 50)\n    print(f\"Translated Text: \\n{translated_text}\")\n    print(\"-\" * 50)\n    print(f\"Final Score (BLEU + Cosine Similarity): {final_score:.4f}\")\n    print(\"-\" * 50)\n\n    # Gather user feedback\n    feedback = input(\"\\nIs the translation good? (yes/no): \").strip().lower()\n    if feedback == \"no\":\n        corrected_translation = input(\"Please provide the correct translation: \").strip()\n\n        # Store feedback\n        feedback_data.append({\"source\": input_text, \"target\": corrected_translation})\n        print(\"Feedback recorded for reinforcement learning.\")\n\n        # Fine-tune model after collecting 3 feedback examples\n        if len(feedback_data) >= 3:\n            print(\"\\nFine-tuning the model with collected feedback...\")\n            fine_tune_with_rl(model, tokenizer, feedback_data)\n            feedback_data.clear()  # Clear feedback after fine-tuning\n    else:\n        print(\"Translation accepted.\")\n\n# Main function\ndef main():\n    global tokenizer_en_fr, model_en_fr\n    tokenizer_en_fr, model_en_fr = load_fine_tuned_model()\n\n    print(\"\\nWelcome to the Reinforcement Learning-Based Translation System!\")\n    print(\"You need to provide at least 3 feedbacks to fine-tune the model.\\n\")\n\n    while True:\n        input_text = input(\"\\nEnter text for translation (or type 'exit' to quit): \").strip()\n        if input_text.lower() == \"exit\":\n            print(\"Exiting the translation system. Goodbye!\")\n            break\n\n        direction = input(\"Enter translation direction (en_fr for English to French, fr_en for French to English): \").strip()\n\n        translate_and_evaluate(input_text, direction)\n\n# Run the system\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:12:00.918438Z","iopub.execute_input":"2025-01-09T12:12:00.918773Z","iopub.status.idle":"2025-01-09T12:23:11.174951Z","shell.execute_reply.started":"2025-01-09T12:12:00.918744Z","shell.execute_reply":"2025-01-09T12:23:11.174029Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b0428db84c645bb96734ed4d47eaaa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded5d9ebe107491489f09a8d50158e1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aed4a4104194092aa126197aae5a2e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b3c929bbe154fda96a5c94facce82c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63e7099f8b5b451ea1c1782301525f33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f338172ef7fd4183a520ed27576ced07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94bb3d431474323baf93ecc69573645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a3e34e5ab249f88f953a9c6d3843df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d4a1631669545a186b9bea0db9fc37c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f1ada3babed459a86b0e2802f30c5a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"765a2eaa01a342d8b27323520a27fa2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23346ed60324538b70ca42341663d19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"213a4bd9b2354c8a9ba7169cf09515e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c662a92b67b9404d8288a6d81b14d117"}},"metadata":{}},{"name":"stdout","text":"\nWelcome to the Reinforcement Learning-Based Translation System!\nYou need to provide at least 3 feedbacks to fine-tune the model.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220225329d8042d5a84c173eadca092a"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"\nEnter text for translation (or type 'exit' to quit):  Any contracting State which, under its domestic law, requires as a condition of respect for copyright formalities such as deposit, registration, notification, notarial certification, payment of fees or manufacture or publication in that contracting State shall consider these conditions fulfilled for all works protected in accordance with this Convention and first published outside its territory and whose author is not one of its nationals, if, from the time of first publication, all copies of the work published with the authority of the author or other copyright owner bear the symbol © accompanied by the name of the copyright owner and the year of first publication placed in the manner and in the place of reasonable notice of the copyright claim.\nEnter translation direction (en_fr for English to French, fr_en for French to English):  en_fr\n"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07f442d13292452bbadb32363c1c60bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd963c0e5cb443d3b357698a9b9a08e8"}},"metadata":{}},{"name":"stdout","text":"\n--- Translation ---\n--------------------------------------------------\nInput Text: \nAny contracting State which, under its domestic law, requires as a condition of respect for copyright formalities such as deposit, registration, notification, notarial certification, payment of fees or manufacture or publication in that contracting State shall consider these conditions fulfilled for all works protected in accordance with this Convention and first published outside its territory and whose author is not one of its nationals, if, from the time of first publication, all copies of the work published with the authority of the author or other copyright owner bear the symbol © accompanied by the name of the copyright owner and the year of first publication placed in the manner and in the place of reasonable notice of the copyright claim.\n--------------------------------------------------\nTranslated Text: \nTout État contractant qui, en vertu de son droit interne, exige comme condition de respect des formalités de droit d'auteur telles que le dépôt, l'enregistrement, la notification, la certification notariée, le paiement des taxes ou de la fabrication ou de la publication dans cet État contractant considère que ces conditions sont remplies pour toutes les oeuvres protégées conformément à la présente Convention et publiées en premier lieu hors de son territoire et dont l'auteur n'est pas l'un de ses ressortissants, si, dès la première publication, toutes les copies de l'oeuvre publiées avec l'autorité de l'auteur ou d'un\n--------------------------------------------------\nFinal Score (BLEU + Cosine Similarity): 0.5188\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nIs the translation good? (yes/no):  yes\n"},{"name":"stdout","text":"Translation accepted.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter text for translation (or type 'exit' to quit):  exit\n"},{"name":"stdout","text":"Exiting the translation system. Goodbye!\n","output_type":"stream"}],"execution_count":13}]}